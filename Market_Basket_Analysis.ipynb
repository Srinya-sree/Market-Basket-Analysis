{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e824fe82",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ›’ Market Basket Analysis (MBA) â€“ Product Recommendations\n",
    "\n",
    "**Author:** Sri Saranya Chandrapati  \n",
    "**Objective:** Discover frequent itemsets and association rules from transactional data and visualize insights.\n",
    "\n",
    "This notebook supports:\n",
    "- Loading your own dataset **or** using the included `sample_transactions.csv`\n",
    "- Data wrangling to build baskets (transactions â†’ list of items)\n",
    "- Apriori frequent itemset mining (pure Python implementation; no external libs required)\n",
    "- Association rules (support, confidence, lift)\n",
    "- Visualizations (matplotlib only)\n",
    "- Export of rules to CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Setup & Configuration\n",
    "# =====================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matplotlib settings (no styles or colors set as requested)\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 5)\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = \"/mnt/data/sample_transactions.csv\"  # replace with your file path if using your own dataset\n",
    "EXPORT_RULES_CSV = \"/mnt/data/mba_rules.csv\"\n",
    "EXPORT_ITEMSETS_CSV = \"/mnt/data/mba_frequent_itemsets.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71807109",
   "metadata": {},
   "source": [
    "## 1. Load Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    df_raw = pd.read_csv(DATA_PATH)\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"Could not read dataset at {DATA_PATH}: {e}\")\n",
    "\n",
    "print(\"Preview:\")\n",
    "display(df_raw.head())\n",
    "print(\"\\nColumns:\", list(df_raw.columns))\n",
    "print(\"\\nRows:\", len(df_raw))\n",
    "\n",
    "# Expecting columns: TransactionID, Item\n",
    "if not set([\"TransactionID\", \"Item\"]).issubset(df_raw.columns):\n",
    "    raise ValueError(\"Dataset must contain 'TransactionID' and 'Item' columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4b834",
   "metadata": {},
   "source": [
    "## 2. Build Baskets (Transaction â†’ List of Items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baskets_df = df_raw.groupby(\"TransactionID\")[\"Item\"].apply(lambda s: list(dict.fromkeys([str(x).strip() for x in s if pd.notnull(x)]))).reset_index()\n",
    "transactions = baskets_df[\"Item\"].tolist()\n",
    "print(f\"Total transactions: {len(transactions)}\")\n",
    "print(\"Sample baskets:\")\n",
    "for i in range(min(5, len(transactions))):\n",
    "    print(f\"{i+1}: {transactions[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6feae6",
   "metadata": {},
   "source": [
    "## 3. Apriori â€“ Frequent Itemset Mining (Pure Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd6817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_support(itemset, transactions):\n",
    "    count = 0\n",
    "    for t in transactions:\n",
    "        if itemset.issubset(set(t)):\n",
    "            count += 1\n",
    "    return count / len(transactions) if transactions else 0.0\n",
    "\n",
    "def generate_candidates(prev_frequents, k):\n",
    "    # Join step: combine itemsets of size k-1 to produce candidates of size k\n",
    "    prev_list = sorted(list(prev_frequents))\n",
    "    candidates = set()\n",
    "    for i in range(len(prev_list)):\n",
    "        for j in range(i+1, len(prev_list)):\n",
    "            a = prev_list[i]\n",
    "            b = prev_list[j]\n",
    "            union = a.union(b)\n",
    "            if len(union) == k:\n",
    "                # Prune step: all (k-1)-subsets must be frequent\n",
    "                all_subsets_frequent = True\n",
    "                for subset in combinations(union, k-1):\n",
    "                    if frozenset(subset) not in prev_frequents:\n",
    "                        all_subsets_frequent = False\n",
    "                        break\n",
    "                if all_subsets_frequent:\n",
    "                    candidates.add(frozenset(union))\n",
    "    return candidates\n",
    "\n",
    "def apriori(transactions, min_support=0.2):\n",
    "    # L1: singletons\n",
    "    item_counts = defaultdict(int)\n",
    "    n = len(transactions)\n",
    "    for t in transactions:\n",
    "        for item in set(t):\n",
    "            item_counts[item] += 1\n",
    "\n",
    "    L1 = set()\n",
    "    supports = {}\n",
    "    for item, cnt in item_counts.items():\n",
    "        sup = cnt / n if n else 0.0\n",
    "        if sup >= min_support:\n",
    "            L1.add(frozenset([item]))\n",
    "            supports[frozenset([item])] = sup\n",
    "\n",
    "    frequents = [L1]\n",
    "    k = 2\n",
    "    while True:\n",
    "        Ck = generate_candidates(frequents[-1], k)\n",
    "        Lk = set()\n",
    "        for c in Ck:\n",
    "            sup = get_support(c, transactions)\n",
    "            if sup >= min_support:\n",
    "                Lk.add(c)\n",
    "                supports[c] = sup\n",
    "        if not Lk:\n",
    "            break\n",
    "        frequents.append(Lk)\n",
    "        k += 1\n",
    "\n",
    "    # Flatten frequent itemsets\n",
    "    all_frequents = set().union(*frequents) if frequents else set()\n",
    "    return all_frequents, supports\n",
    "\n",
    "# Run apriori\n",
    "MIN_SUPPORT = 0.2  # adjust as needed\n",
    "frequent_itemsets, supports_map = apriori(transactions, min_support=MIN_SUPPORT)\n",
    "print(f\"Found {len(frequent_itemsets)} frequent itemsets with min_support={MIN_SUPPORT}.\")\n",
    "# Convert to DataFrame for review/export\n",
    "fi_rows = []\n",
    "for itemset in sorted(frequent_itemsets, key=lambda x: (len(x), sorted(list(x)))):\n",
    "    fi_rows.append({\n",
    "        \"itemset\": \", \".join(sorted(list(itemset))),\n",
    "        \"k\": len(itemset),\n",
    "        \"support\": supports_map[itemset]\n",
    "    })\n",
    "df_fi = pd.DataFrame(fi_rows).sort_values([\"k\", \"support\"], ascending=[True, False]).reset_index(drop=True)\n",
    "display(df_fi.head(20))\n",
    "df_fi.to_csv(EXPORT_ITEMSETS_CSV, index=False)\n",
    "print(f\"Saved frequent itemsets to {EXPORT_ITEMSETS_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd584b9",
   "metadata": {},
   "source": [
    "## 4. Association Rules (Support, Confidence, Lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a839f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def association_rules(frequents, supports, min_confidence=0.5, min_lift=1.0):\n",
    "    rules = []\n",
    "    for itemset in frequents:\n",
    "        if len(itemset) < 2:  # rules need at least 2 items\n",
    "            continue\n",
    "        items = set(itemset)\n",
    "        # For each non-empty proper subset as antecedent\n",
    "        for r in range(1, len(itemset)):\n",
    "            for antecedent in combinations(items, r):\n",
    "                antecedent = frozenset(antecedent)\n",
    "                consequent = itemset - antecedent\n",
    "                if not consequent:\n",
    "                    continue\n",
    "                sup_X = supports.get(antecedent, 0.0)\n",
    "                sup_XY = supports.get(itemset, 0.0)\n",
    "                sup_Y = supports.get(consequent, 0.0)\n",
    "                if sup_X <= 0 or sup_Y <= 0:\n",
    "                    continue\n",
    "                conf = sup_XY / sup_X\n",
    "                lift = conf / sup_Y\n",
    "                if conf >= min_confidence and lift >= min_lift:\n",
    "                    rules.append({\n",
    "                        \"antecedent\": \", \".join(sorted(list(antecedent))),\n",
    "                        \"consequent\": \", \".join(sorted(list(consequent))),\n",
    "                        \"support\": sup_XY,\n",
    "                        \"confidence\": conf,\n",
    "                        \"lift\": lift\n",
    "                    })\n",
    "    rules_df = pd.DataFrame(rules).sort_values([\"lift\", \"confidence\", \"support\"], ascending=False).reset_index(drop=True)\n",
    "    return rules_df\n",
    "\n",
    "MIN_CONFIDENCE = 0.5\n",
    "MIN_LIFT = 1.0\n",
    "rules_df = association_rules(frequent_itemsets, supports_map, min_confidence=MIN_CONFIDENCE, min_lift=MIN_LIFT)\n",
    "print(f\"Generated {len(rules_df)} rules (min_confidence={MIN_CONFIDENCE}, min_lift={MIN_LIFT}).\")\n",
    "display(rules_df.head(20))\n",
    "rules_df.to_csv(EXPORT_RULES_CSV, index=False)\n",
    "print(f\"Saved rules to {EXPORT_RULES_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5ce54",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdfab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5.1 Top-k frequent itemsets by support (k >= 2)\n",
    "df_pairs_or_more = df_fi[df_fi[\"k\"] >= 2].copy()\n",
    "top_n = min(10, len(df_pairs_or_more))\n",
    "top_itemsets = df_pairs_or_more.nlargest(top_n, \"support\")\n",
    "\n",
    "if not top_itemsets.empty:\n",
    "    plt.figure()\n",
    "    plt.barh(range(len(top_itemsets)), top_itemsets[\"support\"])\n",
    "    plt.yticks(range(len(top_itemsets)), top_itemsets[\"itemset\"])\n",
    "    plt.title(\"Top Frequent Itemsets (k >= 2) by Support\")\n",
    "    plt.xlabel(\"Support\")\n",
    "    plt.ylabel(\"Itemset\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No itemsets of size >= 2 at current thresholds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd069c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5.2 Scatter plot: Support vs Confidence (top rules)\n",
    "if not rules_df.empty:\n",
    "    plt.figure()\n",
    "    plt.scatter(rules_df[\"support\"], rules_df[\"confidence\"])\n",
    "    plt.title(\"Rules: Support vs Confidence\")\n",
    "    plt.xlabel(\"Support\")\n",
    "    plt.ylabel(\"Confidence\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No rules to visualize at current thresholds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc434e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5.3 Scatter plot: Confidence vs Lift (top rules)\n",
    "if not rules_df.empty:\n",
    "    plt.figure()\n",
    "    plt.scatter(rules_df[\"confidence\"], rules_df[\"lift\"])\n",
    "    plt.title(\"Rules: Confidence vs Lift\")\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Lift\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No rules to visualize at current thresholds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5e006",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Use This Notebook With Your Own Data\n",
    "\n",
    "Your CSV must have **two columns** (any order, any names are fine, just rename in the cell):\n",
    "- A transaction identifier (e.g., `TransactionID`, `InvoiceNo`, etc.)\n",
    "- An item / product name (e.g., `Item`, `Description`, etc.)\n",
    "\n",
    "**Steps:**\n",
    "1. Upload your CSV to this environment or place it in your project folder.\n",
    "2. Change `DATA_PATH` at the top to your file path, for example:\n",
    "   ```python\n",
    "   DATA_PATH = \"/mnt/data/OnlineRetail.csv\"\n",
    "   ```\n",
    "3. If your column names differ, rename them before building baskets:\n",
    "   ```python\n",
    "   df_raw = pd.read_csv(DATA_PATH)\n",
    "   df_raw = df_raw.rename(columns={\"InvoiceNo\":\"TransactionID\", \"Description\":\"Item\"})\n",
    "   ```\n",
    "4. Tune thresholds:\n",
    "   ```python\n",
    "   MIN_SUPPORT = 0.02\n",
    "   MIN_CONFIDENCE = 0.6\n",
    "   MIN_LIFT = 1.2\n",
    "   ```\n",
    "5. Re-run the notebook. Exports:\n",
    "   - Frequent Itemsets â†’ `/mnt/data/mba_frequent_itemsets.csv`\n",
    "   - Rules â†’ `/mnt/data/mba_rules.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d90c2b",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Resume Bullet (Copy-Paste)\n",
    "\n",
    "**Market Basket Analysis** â€“ Implemented Apriori in Python to mine frequent itemsets and generate association rules from retail transaction data. Built matplotlib visualizations and exported rules, uncovering product bundles that inform crossâ€‘sell strategies.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
